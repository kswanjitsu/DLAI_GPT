{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "set_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# optimizer\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in t5_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-4, eps=1e-8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "hypernym_finder_tuples = [(\"The hypernym for red is\",\"The hypernym for red is color\"),\n",
    "                          (\"The hypernym for green is\",\"The hypernym for green is color\"),\n",
    "                          (\"The hypernym for yellow is\",\"The hypernym for yellow is color\"),\n",
    "                          (\"The hypernym for hydrogen peroxide is\",\"The hypernym for hydrogen peroxide is molecule\"),\n",
    "                          (\"The hypernym for hydrogen peroxide is\",\"The hypernym for hydrogen peroxide is chemical\"),\n",
    "                          (\"The hypernym for heart is\",\"The hypernym for heart is body part\"),\n",
    "                          (\"The hypernym for heart is\",\"The hypernym for heart is anatomical structure\"),\n",
    "                          (\"The hypernym for bipolar is\",\"The hypernym for bipolar is mood disorder\"),\n",
    "                          (\"The hypernym for bipolar is\",\"The hypernym for bipolar is mental illness\"),\n",
    "                          (\"The hypernym for eukaryote is\",\"The hypernym for x is y\"),\n",
    "                          (\"The hypernym for Asia is\",\"The hypernym for Asia is Continent\"),\n",
    "                          (\"The hypernym for Europe is\",\"The hypernym for Europe is Continent\"),\n",
    "                          (\"The hypernym for pitbull is\",\"The hypernym for pitbull is dog\"),\n",
    "                          (\"The hypernym for german shepherd is\",\"The hypernym for german shepherd is dog\"),\n",
    "                          (\"The hypernym for greyhound is\",\"The hypernym for greyhound is dog\"),\n",
    "                          (\"The hypernym of atrial fibrillation is\",\"The hypernym of atrial fibrillation is arrhythmia\"),\n",
    "                          (\"The hypernym of ventricular tachycardia is\",\"The hypernym of ventricular tachycardia is arrhythmia\"),\n",
    "                          (\"The hypernym for antidepressant is\",\"drug\"),\n",
    "                          (\"The hypernym for amlodipine is\",\"drug\"),\n",
    "                          (\"The hypernym for doxorubicin is\",\"chemotherapy\"),\n",
    "                          (\"The hypernym for diabetes is\",\"The hypernym for diabetes is disease\"),\n",
    "                          (\"The hypernym for polymyalgia rheumatica is\",\"The hypernym for polymyalgia rheumatica is disease\"),\n",
    "                          (\"The hypernym for myocardial infarction is\",\"The hypernym for myocardial infarction is heart attack\"),\n",
    "                          (\"The hypernym for serotonin is\",\"The hypernym for serotonin is molecule\"),\n",
    "                          (\"The hypernym for nucleotide is\",\"The hypernym for nucleotide is molecule\"),\n",
    "                          (\"The hypernym for antifibrinolytic is\",\"The hypernym for antifibrinolytic is blood thinner\"),\n",
    "                          (\"The hypernym for plavix is\",\"The hypernym for plavix is blood thinner\"),\n",
    "                          (\"The hypernym for rivaroxaban is\",\"The hypernym for rivaroxaban is blood thinner\"),\n",
    "                          (\"The hypernym for forceps is\",\"The hypernym for forceps is tool\"),\n",
    "                          (\"The hypernym for wrench is\",\"The hypernym for wrench is tool\"),\n",
    "                          (\"The hypernym for lysine is\",\"The hypernym for lysine is amino acid\"),\n",
    "                          (\"The hypernym for lysine is\",\"The hypernym for lysine is molecule\"),\n",
    "                          (\"The hypernym for atria is\",\"The hypernym for atria is heart chamber\"),\n",
    "                          (\"The hypernym for atria is\",\"The hypernym for atria is body part\"),\n",
    "                          (\"The hypernym for duodenum is\",\"The hypernym for duodenum is body part\"),\n",
    "                          (\"The hypernym for keppra is\",\"The hypernym for keppra is antiepileptic\"),\n",
    "                          (\"The hypernym for keppra is\",\"The hypernym for keppra is antiepileptic\"),\n",
    "                          (\"The hypernym for predecessor is\",\"The hypernym for predecessor is precursor\"),\n",
    "                          (\"The hypernym for president is\",\"The hypernym for president is leader\"),\n",
    "                          (\"The hypernym for letter is\",\"The hypernym for letter is character\"),\n",
    "                          (\"The hypernym for apple is\",\"The hypernym for apple is fruit\"),\n",
    "                          (\"The hypernym for orange is\",\"The hypernym for orange is fruit\"),\n",
    "                          (\"The hypernym for orange is\",\"The hypernym for orange is color\"),\n",
    "                          ]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0\n",
      "epoch  1\n",
      "epoch  2\n",
      "epoch  3\n",
      "epoch  4\n",
      "epoch  5\n",
      "epoch  6\n",
      "epoch  7\n",
      "epoch  8\n",
      "epoch  9\n",
      "epoch  10\n",
      "epoch  11\n",
      "epoch  12\n",
      "epoch  13\n",
      "epoch  14\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "t5_model.train()\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print (\"epoch \",epoch)\n",
    "  for input,output in hypernym_finder_tuples:\n",
    "    input_sent = \"find hypernym: \"+input+ \" </s>\"\n",
    "    ouput_sent = output+\" </s>\"\n",
    "\n",
    "    tokenized_inp = tokenizer.encode_plus(input_sent,  max_length=96, pad_to_max_length=True,return_tensors=\"pt\")\n",
    "    tokenized_output = tokenizer.encode_plus(ouput_sent, max_length=96, pad_to_max_length=True,return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "    input_ids  = tokenized_inp[\"input_ids\"]\n",
    "    attention_mask = tokenized_inp[\"attention_mask\"]\n",
    "\n",
    "    labels= tokenized_output[\"input_ids\"]\n",
    "    decoder_attention_mask=  tokenized_output[\"attention_mask\"]\n",
    "\n",
    "\n",
    "    # the forward function automatically creates the correct decoder_input_ids\n",
    "    output = t5_model(input_ids=input_ids, labels=labels,decoder_attention_mask=decoder_attention_mask,attention_mask=attention_mask)\n",
    "    loss = output[0]\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable word_embeddings already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/home/karl/PycharmProjects/DLAI/translate/labeler.py\", line 118, in construct_network\n    self.word_embeddings = tf.get_variable(\"word_embeddings\",\n  File \"/home/karl/PycharmProjects/DLAI/translate/labeler.py\", line 458, in load\n    labeler.construct_network()\n  File \"/home/karl/PycharmProjects/DLAI/translate/complex_labeller.py\", line 21, in __init__\n    self.model = labeler.SequenceLabeler.load(self.model_path)\n  File \"/tmp/ipykernel_54745/2995099527.py\", line 5, in <module>\n    model = Complexity_labeller(model_path, temp_path)\n  File \"/home/karl/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_54745/3876898622.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mmodel_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'./cwi_seq.model'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mtemp_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'./temp_file.txt'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mComplexity_labeller\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;31m#2044\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/DLAI/translate/complex_labeller.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, model_path, temp_file)\u001B[0m\n\u001B[1;32m     19\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_path\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_path\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtemp_file\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtemp_file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 21\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlabeler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSequenceLabeler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     22\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredictions_cache\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/DLAI/translate/labeler.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(filename)\u001B[0m\n\u001B[1;32m    456\u001B[0m             \u001B[0mlabeler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msingletons\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdump\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"singletons\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    457\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 458\u001B[0;31m             \u001B[0mlabeler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconstruct_network\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    459\u001B[0m             \u001B[0mlabeler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitialize_session\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    460\u001B[0m             \u001B[0mlabeler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/DLAI/translate/labeler.py\u001B[0m in \u001B[0;36mconstruct_network\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    116\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Unknown initializer\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 118\u001B[0;31m         self.word_embeddings = tf.get_variable(\"word_embeddings\", \n\u001B[0m\u001B[1;32m    119\u001B[0m             \u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mword2id\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"word_embedding_size\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    120\u001B[0m             \u001B[0minitializer\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzeros_initializer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"emb_initial_zero\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;32mTrue\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minitializer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001B[0m in \u001B[0;36mget_variable\u001B[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001B[0m\n\u001B[1;32m   1577\u001B[0m                  \u001B[0msynchronization\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mVariableSynchronization\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mAUTO\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1578\u001B[0m                  aggregation=VariableAggregation.NONE):\n\u001B[0;32m-> 1579\u001B[0;31m   return get_variable_scope().get_variable(\n\u001B[0m\u001B[1;32m   1580\u001B[0m       \u001B[0m_get_default_variable_store\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1581\u001B[0m       \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001B[0m in \u001B[0;36mget_variable\u001B[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001B[0m\n\u001B[1;32m   1320\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mdtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1321\u001B[0m         \u001B[0mdtype\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dtype\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1322\u001B[0;31m       return var_store.get_variable(\n\u001B[0m\u001B[1;32m   1323\u001B[0m           \u001B[0mfull_name\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1324\u001B[0m           \u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001B[0m in \u001B[0;36mget_variable\u001B[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001B[0m\n\u001B[1;32m    576\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mcustom_getter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0mcustom_getter_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    577\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 578\u001B[0;31m       return _true_getter(\n\u001B[0m\u001B[1;32m    579\u001B[0m           \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    580\u001B[0m           \u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001B[0m in \u001B[0;36m_true_getter\u001B[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001B[0m\n\u001B[1;32m    529\u001B[0m             \"name was already created with partitioning?\" % name)\n\u001B[1;32m    530\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 531\u001B[0;31m       return self._get_single_variable(\n\u001B[0m\u001B[1;32m    532\u001B[0m           \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    533\u001B[0m           \u001B[0mshape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001B[0m in \u001B[0;36m_get_single_variable\u001B[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001B[0m\n\u001B[1;32m    892\u001B[0m         \u001B[0;31m# default case.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    893\u001B[0m         \u001B[0mtb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mx\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtb\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0;34m\"tensorflow/python\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 894\u001B[0;31m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001B[0m\u001B[1;32m    895\u001B[0m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001B[1;32m    896\u001B[0m       \u001B[0mfound_var\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_vars\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: Variable word_embeddings already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/home/karl/PycharmProjects/DLAI/translate/labeler.py\", line 118, in construct_network\n    self.word_embeddings = tf.get_variable(\"word_embeddings\",\n  File \"/home/karl/PycharmProjects/DLAI/translate/labeler.py\", line 458, in load\n    labeler.construct_network()\n  File \"/home/karl/PycharmProjects/DLAI/translate/complex_labeller.py\", line 21, in __init__\n    self.model = labeler.SequenceLabeler.load(self.model_path)\n  File \"/tmp/ipykernel_54745/2995099527.py\", line 5, in <module>\n    model = Complexity_labeller(model_path, temp_path)\n  File \"/home/karl/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Import CWI modules and point machine path to temporary path that the CWI module uses to do work\n",
    "from complex_labeller import Complexity_labeller\n",
    "model_path = './cwi_seq.model'\n",
    "temp_path = './temp_file.txt'\n",
    "model = Complexity_labeller(model_path, temp_path)\n",
    "#2044\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def autotranslate(document):\n",
    "    return_list = []\n",
    "    #print('\\n#####################NEW ENTRY#######################')\n",
    "    #print(cui)\n",
    "    old_document = str(document)\n",
    "    new_document = str(document)\n",
    "    cw_list = []\n",
    "\n",
    "    #try:\n",
    "    Complexity_labeller.convert_format_string(model, document)\n",
    "    dataframe = Complexity_labeller.get_dataframe(model)\n",
    "    cw_list = list(zip(dataframe['sentences'].values[0], dataframe['labels'].values[0], dataframe['probs'].values[0]))\n",
    "    #except:\n",
    "    #pass\n",
    "    print(cw_list)\n",
    "    for i in cw_list:\n",
    "        if i[1] == 1:\n",
    "\n",
    "            test_sent = 'find hypernym: The hypernym for {} is</s>'.format(str(i[0]))\n",
    "            test_tokenized = tokenizer.encode_plus(test_sent, return_tensors=\"pt\")\n",
    "\n",
    "            test_input_ids  = test_tokenized[\"input_ids\"]\n",
    "            test_attention_mask = test_tokenized[\"attention_mask\"]\n",
    "\n",
    "            t5_model.eval()\n",
    "            beam_outputs = t5_model.generate(\n",
    "                input_ids=test_input_ids,attention_mask=test_attention_mask,\n",
    "                max_length=60,\n",
    "                early_stopping=True,\n",
    "                num_beams=10,\n",
    "                num_return_sequences=1,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "\n",
    "            for beam_output in beam_outputs:\n",
    "                sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "                print(sent)\n",
    "                new_document = old_document.replace(str(i[0]), str(sent.split(' ')[-1]))\n",
    "                old_document = new_document\n",
    "    return document, new_document\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "documents = ['precursor of serotonin used as antiepileptic and antidepressant.',\n",
    "             'Salts and esters of ALGINIC ACID that are used as HYDROGELS; DENTAL IMPRESSION MATERIALS, and as absorbent materials for surgical dressings (BANDAGES, HYDROCOLLOID).',\n",
    "             'A state due to excess loss of carbon dioxide from the body.']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('precursor', 1, array([0.10260871, 0.89739126], dtype=float32)), ('of', 0, array([9.999379e-01, 6.214276e-05], dtype=float32)), ('serotonin', 1, array([0.21891482, 0.7810852 ], dtype=float32)), ('used', 0, array([9.992920e-01, 7.079054e-04], dtype=float32)), ('as', 0, array([9.9993837e-01, 6.1578088e-05], dtype=float32)), ('antiepileptic', 1, array([0.02242535, 0.97757465], dtype=float32)), ('and', 0, array([9.999101e-01, 8.985217e-05], dtype=float32)), ('antidepressant', 1, array([0.03892209, 0.9610779 ], dtype=float32)), ('.', 0, array([9.9995625e-01, 4.3727112e-05], dtype=float32))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karl/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:190: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hypernym for precursor is precursor\n",
      "The hypernym for serotonin is molecule\n",
      "drug\n",
      "drug\n",
      "precursor of serotonin used as antiepileptic and antidepressant. \n",
      " precursor of molecule used as drug and drug.\n",
      "[('Salts', 1, array([0.25910294, 0.7408971 ], dtype=float32)), ('and', 0, array([9.9987948e-01, 1.2050271e-04], dtype=float32)), ('esters', 1, array([0.2572822 , 0.74271786], dtype=float32)), ('of', 0, array([9.9994743e-01, 5.2535663e-05], dtype=float32)), ('ALGINIC', 0, array([0.9330063, 0.0669937], dtype=float32)), ('ACID', 0, array([0.97299373, 0.02700627], dtype=float32)), ('that', 0, array([9.9992144e-01, 7.8585384e-05], dtype=float32)), ('are', 0, array([9.9993289e-01, 6.7087574e-05], dtype=float32)), ('used', 0, array([0.9986003 , 0.00139964], dtype=float32)), ('as', 0, array([9.9993026e-01, 6.9689435e-05], dtype=float32)), ('HYDROGELS', 0, array([0.9472308 , 0.05276922], dtype=float32)), (';', 0, array([9.9995840e-01, 4.1616262e-05], dtype=float32)), ('DENTAL', 0, array([0.9404594 , 0.05954062], dtype=float32)), ('IMPRESSION', 0, array([0.8643407 , 0.13565926], dtype=float32)), ('MATERIALS', 0, array([0.944982  , 0.05501806], dtype=float32)), (',', 0, array([9.9995828e-01, 4.1697338e-05], dtype=float32)), ('and', 0, array([9.9994218e-01, 5.7810754e-05], dtype=float32)), ('as', 0, array([9.9993384e-01, 6.6105895e-05], dtype=float32)), ('absorbent', 1, array([0.1884394 , 0.81156063], dtype=float32)), ('materials', 1, array([0.16483575, 0.8351643 ], dtype=float32)), ('for', 0, array([9.9995959e-01, 4.0398467e-05], dtype=float32)), ('surgical', 1, array([0.16030319, 0.8396968 ], dtype=float32)), ('dressings', 1, array([0.30726716, 0.6927328 ], dtype=float32)), ('(', 1, array([0.4869376 , 0.51306236], dtype=float32)), ('BANDAGES', 0, array([0.955121  , 0.04487898], dtype=float32)), (',', 0, array([9.9996030e-01, 3.9712217e-05], dtype=float32)), ('HYDROCOLLOID', 0, array([0.9750224 , 0.02497765], dtype=float32)), (')', 1, array([0.3530971 , 0.64690286], dtype=float32)), ('.', 0, array([9.999604e-01, 3.958306e-05], dtype=float32))]\n",
      "The hypernym for Salts is color\n",
      "The hypernym for esters is molecule\n",
      "The hypernym for absorbent is substance\n",
      "The hypernym for materials is molecule\n",
      "The hypernym for surgical is surgery\n",
      "The hypernym for dressings is blood thinner\n",
      "The hypernym for ( is body part\n",
      "The hypernym for ) is body part\n",
      "Salts and esters of ALGINIC ACID that are used as HYDROGELS; DENTAL IMPRESSION MATERIALS, and as absorbent materials for surgical dressings (BANDAGES, HYDROCOLLOID). \n",
      " color and molecule of ALGINIC ACID that are used as HYDROGELS; DENTAL IMPRESSION MATERIALS, and as substance molecule for surgery thinner partBANDAGES, HYDROCOLLOIDpart.\n",
      "[('A', 0, array([9.9982113e-01, 1.7890493e-04], dtype=float32)), ('state', 0, array([0.9950395 , 0.00496052], dtype=float32)), ('due', 0, array([0.9191058 , 0.08089413], dtype=float32)), ('to', 0, array([9.9992955e-01, 7.0499300e-05], dtype=float32)), ('excess', 1, array([0.12397844, 0.87602156], dtype=float32)), ('loss', 0, array([0.9160294 , 0.08397058], dtype=float32)), ('of', 0, array([9.9995553e-01, 4.4435255e-05], dtype=float32)), ('carbon', 0, array([0.9323305 , 0.06766948], dtype=float32)), ('dioxide', 1, array([0.14197749, 0.8580225 ], dtype=float32)), ('from', 0, array([9.9993527e-01, 6.4670479e-05], dtype=float32)), ('the', 0, array([9.9991024e-01, 8.9723479e-05], dtype=float32)), ('body', 0, array([0.99259573, 0.00740425], dtype=float32)), ('.', 0, array([9.9995518e-01, 4.4803743e-05], dtype=float32))]\n",
      "The hypernym for excess is calorie\n",
      "The hypernym for dioxide is chemical\n",
      "A state due to excess loss of carbon dioxide from the body. \n",
      " A state due to calorie loss of carbon chemical from the body.\n"
     ]
    }
   ],
   "source": [
    "for document in documents:\n",
    "    old_doc, new_doc = autotranslate(document)\n",
    "    print(old_doc,'\\n', new_doc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                # You stopped here. You were thinking about running this on a few examples and tidying up the function to replace the\n",
    "# actual words. May need a few more \"shots\" and a few more epochs for accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# From here on in the code were tests - can ignore, or play\n",
    "#pip uninstall transformers\n",
    "#pip install git+https://github.com/StellaAthena/transformers\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 11, but ``max_length`` is set to 10. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "data": {
      "text/plain": "[{'generated_text': 'The hypernym for atrial fibrillation is at'}]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"The hypernym for atrial fibrillation is\", do_sample=True, max_length=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import nlpcloud"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "client = nlpcloud.Client(\"gpt-j\", \"your_token\", gpu=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://api.nlpcloud.io/v1/gpu/gpt-j/generation: ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/nlpcloud/__init__.py\u001B[0m in \u001B[0;36mgeneration\u001B[0;34m(self, text, min_length, max_length, length_no_input, end_sequence, remove_input, do_sample, num_beams, early_stopping, no_repeat_ngram_size, num_return_sequences, top_k, top_p, temperature, repetition_penalty, length_penalty, bad_words)\u001B[0m\n\u001B[1;32m     84\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m             \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_for_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mHTTPError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/requests/models.py\u001B[0m in \u001B[0;36mraise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    942\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhttp_error_msg\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 943\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhttp_error_msg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresponse\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    944\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://api.nlpcloud.io/v1/gpu/gpt-j/generation",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_12378/1963228047.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m generation = client.generation(\"\"\"Context: The heart's upper chambers (atria) beat out of coordination with the lower chambers (ventricles). This condition may have no symptoms, but when symptoms do appear they include palpitations, shortness of breath, and fatigue. Treatments include drugs, electrical shock (cardioversion), and minimally invasive surgery (ablation).\n\u001B[0m\u001B[1;32m      2\u001B[0m             \u001B[0mQuestion\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mWhat\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0mgeneral\u001B[0m \u001B[0mhypernym\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0matrial\u001B[0m \u001B[0mfibrillation\u001B[0m \u001B[0mthat\u001B[0m \u001B[0man\u001B[0m \u001B[0;36m8\u001B[0m\u001B[0mth\u001B[0m \u001B[0mgrader\u001B[0m \u001B[0mcan\u001B[0m \u001B[0munderstand\u001B[0m\u001B[0;31m?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m             \u001B[0mAnswer\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0marrhythmia\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m             \u001B[0;31m###\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m             \u001B[0mContext\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDoxorubicin\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtype\u001B[0m \u001B[0mof\u001B[0m \u001B[0mchemotherapy\u001B[0m \u001B[0mdrug\u001B[0m \u001B[0mcalled\u001B[0m \u001B[0man\u001B[0m \u001B[0manthracycline\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mIt\u001B[0m \u001B[0mslows\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mstops\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mgrowth\u001B[0m \u001B[0mof\u001B[0m \u001B[0mcancer\u001B[0m \u001B[0mcells\u001B[0m \u001B[0mby\u001B[0m \u001B[0mblocking\u001B[0m \u001B[0man\u001B[0m \u001B[0menzyme\u001B[0m \u001B[0mcalled\u001B[0m \u001B[0mtopo\u001B[0m \u001B[0misomerase\u001B[0m \u001B[0;36m2.\u001B[0m \u001B[0mCancer\u001B[0m \u001B[0mcells\u001B[0m \u001B[0mneed\u001B[0m \u001B[0mthis\u001B[0m \u001B[0menzyme\u001B[0m \u001B[0mto\u001B[0m \u001B[0mdivide\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mgrow\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mYou\u001B[0m \u001B[0mmight\u001B[0m \u001B[0mhave\u001B[0m \u001B[0mdoxorubicin\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcombination\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mother\u001B[0m \u001B[0mchemotherapy\u001B[0m \u001B[0mdrugs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/rapids-21.10/lib/python3.8/site-packages/nlpcloud/__init__.py\u001B[0m in \u001B[0;36mgeneration\u001B[0;34m(self, text, min_length, max_length, length_no_input, end_sequence, remove_input, do_sample, num_beams, early_stopping, no_repeat_ngram_size, num_return_sequences, top_k, top_p, temperature, repetition_penalty, length_penalty, bad_words)\u001B[0m\n\u001B[1;32m     88\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 90\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\": \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     91\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mr\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjson\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mHTTPError\u001B[0m: 401 Client Error: Unauthorized for url: https://api.nlpcloud.io/v1/gpu/gpt-j/generation: "
     ]
    }
   ],
   "source": [
    "generation = client.generation(\"\"\"Context: The heart's upper chambers (atria) beat out of coordination with the lower chambers (ventricles). This condition may have no symptoms, but when symptoms do appear they include palpitations, shortness of breath, and fatigue. Treatments include drugs, electrical shock (cardioversion), and minimally invasive surgery (ablation).\n",
    "            Question: What is a general hypernym for atrial fibrillation that an 8th grader can understand?\n",
    "            Answer: arrhythmia\n",
    "            ###\n",
    "            Context: Doxorubicin is a type of chemotherapy drug called an anthracycline. It slows or stops the growth of cancer cells by blocking an enzyme called topo isomerase 2. Cancer cells need this enzyme to divide and grow. You might have doxorubicin in combination with other chemotherapy drugs.\n",
    "            Question: What is a general hypernym for doxorubicin that an 8th grader can understand?\n",
    "            Answer: Chemotherapy\n",
    "            ###\n",
    "            Context: Diabetes is a chronic (long-lasting) health condition that affects how your body turns food into energy. Most of the food you eat is broken down into sugar (also called glucose) and released into your bloodstream. When your blood sugar goes up, it signals your pancreas to release insulin\n",
    "            Question: What is a general hypernym for diabetes that an 8th grader can understand?\n",
    "            Answer:\"\"\",\n",
    "    length_no_input=True,\n",
    "    end_sequence=\"\\n###\",\n",
    "    remove_input=True)\n",
    "print(generation[\"generated_text\"])\n",
    "\n",
    "#  Use mostly wn for hypernym\n",
    "# Need to build out cosine similarity for hypernym defs\n",
    "# If no sub possible with WN - then use UMLS\n",
    "# May be best to reconsitute sentence after wn sub - rerun CWI - then run biomedical translator? probs not"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from translate_functions import find_complex_words, wn_hypernym_sub, sub_umls_hypernym, recon_doc.py,\n",
    "\n",
    "#from sub_umls_hypernym import sub_umls_hypernym\n",
    "#from recon_doc import recon_doc\n",
    "#from runGinger import runGinger\n",
    "#from grade_the_document import grade_the_document"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"GPT Is a powerful tool, but it is known to give falsehoods. This falsehood return is worse with larger models\n",
    "as the model learns the way humans give falsehood answers. https://twitter.com/owainevans_uk/status/1438472786188636162\n",
    "For this reason, we believe that few-shot training at the very least and specific parameters of what the model should do\n",
    "is necessary for use in biomedical text dejargonization\"\"\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for line in open('/home/karl/PycharmProjects/DLAI/datasets/UMLS/MRDEF_test.txt'):\n",
    "    splitline = line.split('|')\n",
    "    cui = splitline[0]\n",
    "    document = splitline[1]\n",
    "    cw_list = find_complex_words(document)\n",
    "    cw_list = wn_hypernym_sub(cw_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-aac303ae",
   "language": "python",
   "display_name": "PyCharm (DLAI)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}